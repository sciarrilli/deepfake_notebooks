{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Anomaly Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "detector = MTCNN(margin=40, select_largest=False, image_size=224, device=device, post_process=False)\n",
    "\n",
    "# list of videos\n",
    "video_path = '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos'\n",
    "video_fnames = os.listdir(video_path)\n",
    "videos = []\n",
    "for fname in video_fnames:\n",
    "    videos.append(os.path.join(video_path, fname))\n",
    "\n",
    "\n",
    "# set output path\n",
    "path = '/home/ec2-user/SageMaker/data/frames_17x'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# frame indices to grab\n",
    "frame_idxs = [i for i in range(30, 301, 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata_file = '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/metadata.json'\n",
    "\n",
    "metadata_df = pd.read_json(metadata_file).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>None</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label        original  split\n",
       "aagfhgtpmv.mp4  FAKE  vudstovrck.mp4  train\n",
       "aapnvogymq.mp4  FAKE  jdubbvfswz.mp4  train\n",
       "abarnvbtwb.mp4  REAL            None  train\n",
       "abofeumbvv.mp4  FAKE  atvmxvwyns.mp4  train\n",
       "abqwwspghj.mp4  FAKE  qzimuostzz.mp4  train"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dkwjwbwgey.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>rfzzrftgco.mp4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label        original  split\n",
       "dkwjwbwgey.mp4  FAKE  rfzzrftgco.mp4  train"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df[metadata_df.index == 'dkwjwbwgey.mp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = metadata_df[metadata_df.label == \"REAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abarnvbtwb.mp4'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = metadata_df[metadata_df.label == \"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aagfhgtpmv.mp4'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 323)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_df), len(fake_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 60, 90, 120, 150, 180, 210, 240, 270, 300]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_at_indices(videos, frame_idxs):\n",
    "    images = {}\n",
    "    for video in videos:\n",
    "        for frame_num in frame_idxs:\n",
    "            images.update(grab_frames(video, frame_num))\n",
    "    return images\n",
    "\n",
    "\n",
    "def grab_frames(video, frame_num):\n",
    "    #video = os.path.join(video_dir, sample)\n",
    "    filename = video[:-4]+'_'+ str(frame_num) +'.jpg'\n",
    "    reader = cv2.VideoCapture(video)\n",
    "    reader.set(1, frame_num)\n",
    "    _, image = reader.read()\n",
    "    images = {}\n",
    "    images[filename] = image\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # cv2.imwrite(filename, image)\n",
    "    #images_dict = {}\n",
    "    #images_dict[filename] = image\n",
    "    reader.release()\n",
    "    return images\n",
    "\n",
    "\n",
    "def multiprocess_read_frames_at_indices(videos, frame_idxs, job_num):\n",
    "    results = Parallel(n_jobs=job_num)(delayed(grab_frames)(video, frame_num) \n",
    "        for video in videos for frame_num in frame_idxs)\n",
    "    images = {}\n",
    "    for item in results:\n",
    "        images.update(item)\n",
    "    return images\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images):\n",
    "    faces = {}\n",
    "    for key in images.keys():\n",
    "        try:\n",
    "            imgs_pil = Image.fromarray(images[key])\n",
    "            faces.update( {key: detector(imgs_pil)} )\n",
    "        except:\n",
    "            pass\n",
    "    return faces\n",
    "\n",
    "\n",
    "def write_images_to_disk(path, faces):\n",
    "    for face in faces.keys():\n",
    "        try:\n",
    "            image = faces[face].permute(1, 2, 0).int().numpy()\n",
    "            jpg = face.split('/')[-1]\n",
    "            filename = os.path.join(path, jpg)\n",
    "            cv2.imwrite(filename, image)\n",
    "        except:\n",
    "            faces[face] = 'no face detected'\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single processing\n",
    "# images = read_frames_at_indices(videos, frame_idxs)\n",
    "\n",
    "# for multiprocessing\n",
    "images = multiprocess_read_frames_at_indices(videos, frame_idxs, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/cyboodqqyr.mp4'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vids = videos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/cyboodqqyr.mp4',\n",
       " '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey.mp4',\n",
       " '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/esckbnkkvb.mp4',\n",
       " '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/cttqtsjvgn.mp4',\n",
       " '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dzqwgqewhu.mp4']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = read_frames_at_indices[sample_vids, frame_idxs]\n",
    "\n",
    "images = {}\n",
    "for frame_num in frame_idxs:\n",
    "    images.update(grab_frames(videos[1], frame_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = detect_facenet_pytorch(detector, images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_faces = faces.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_30.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_60.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_90.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_120.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_150.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_180.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_210.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_240.jpg', '/home/ec2-user/SageMaker/data/deepfake-samples/sample/train_sample_videos/dkwjwbwgey_270.jpg'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_avg_pixels = []\n",
    "for face in list_of_faces:\n",
    "    try:\n",
    "        frame_avg_pixels.append(torch.mean(faces[face]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(76.3063),\n",
       " tensor(75.9641),\n",
       " tensor(75.9244),\n",
       " tensor(77.3486),\n",
       " tensor(77.5707),\n",
       " tensor(77.7624),\n",
       " tensor(76.4414),\n",
       " tensor(76.8857),\n",
       " tensor(78.5662)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_avg_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(93.9107),\n",
       " tensor(90.0922),\n",
       " tensor(92.0537),\n",
       " tensor(93.2568),\n",
       " tensor(92.9295),\n",
       " tensor(92.6925),\n",
       " tensor(91.7141),\n",
       " tensor(94.6976),\n",
       " tensor(93.4106)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REAL video sample\n",
    "\n",
    "images = {}\n",
    "for frame_num in frame_idxs:\n",
    "    full_path = os.path.join(video_path, real_df.index[8])\n",
    "    images.update(grab_frames(full_path, frame_num))\n",
    "\n",
    "faces = detect_facenet_pytorch(detector, images)\n",
    "\n",
    "list_of_faces = faces.keys()\n",
    "frame_avg_pixels = []\n",
    "for face in list_of_faces:\n",
    "    try:\n",
    "        frame_avg_pixels.append(torch.mean(faces[face]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "print(len(frame_avg_pixels))\n",
    "frame_avg_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAKE video sample\n",
    "\n",
    "images = {}\n",
    "for frame_num in frame_idxs:\n",
    "    full_path = os.path.join(video_path, fake_df.index[8])\n",
    "    images.update(grab_frames(full_path, frame_num))\n",
    "\n",
    "faces = detect_facenet_pytorch(detector, images)\n",
    "\n",
    "list_of_faces = faces.keys()\n",
    "frame_avg_pixels = []\n",
    "for face in list_of_faces:\n",
    "    try:\n",
    "        frame_avg_pixels.append(torch.mean(faces[face]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(len(frame_avg_pixels))\n",
    "frame_avg_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__array_interface__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-160e27f94541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# detect faces and crop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_facenet_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-4d56266e1ff7>\u001b[0m in \u001b[0;36mdetect_facenet_pytorch\u001b[0;34m(detector, images)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mimgs_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_pil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2451\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     \"\"\"\n\u001b[0;32m-> 2453\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2454\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__array_interface__'"
     ]
    }
   ],
   "source": [
    "# detect faces and crop\n",
    "faces = detect_facenet_pytorch(detector, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save to disk\n",
    "write_images_to_disk(path, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport boto3\\n\\nsns = boto3.client('sns')\\nresponse = sns.publish(\\n    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\\n    Message='finished processing v3'\\n)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message='finished processing v3'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
