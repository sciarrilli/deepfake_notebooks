{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------  \n",
    "\n",
    "### Preprocessing Video\n",
    "\n",
    "Working on streamlining the preprocessing. Will attempt to pull frame 100 from the entire dataset. Plan is to use cv2 to grab the frame and save the frame to a list. Then use facenet to detect face and crop on face. And then save to disk. \n",
    "\n",
    "<img style=\"float: center;\" src=\"deepfake.jpg\">\n",
    "\n",
    "-------------------------------------------------------------------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pypi https://pypi.org/project/facenet-pytorch/\n",
    "#\n",
    "# install facenet-pytorch on kaggle without internet\n",
    "# !pip install ../facenet-pytorch/facenet_pytorch-2.0.0-py3-none-any.whl --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting facenet_pytorch\n",
      "  Using cached https://files.pythonhosted.org/packages/ce/60/5192979f70b14681c698f61aace14e906bc92abc0790b0002ebd017dd3d3/facenet_pytorch-2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from facenet_pytorch) (2.22.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from facenet_pytorch) (1.17.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from requests->facenet_pytorch) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from requests->facenet_pytorch) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from requests->facenet_pytorch) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from requests->facenet_pytorch) (2019.11.28)\n",
      "Installing collected packages: facenet-pytorch\n",
      "Successfully installed facenet-pytorch-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 18\n",
    "# frame number to grab from videos\n",
    "frame_num = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single process cv2\n",
    "'''\n",
    "def grab_frames(sample, frame_num):\n",
    "    video = os.path.join(video_dir, sample)\n",
    "    reader = cv2.VideoCapture(video)\n",
    "    reader.set(1, frame_num)\n",
    "    _, image = reader.read()\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    reader.release()\n",
    "    return image\n",
    "\n",
    "\n",
    "images_dict= {}\n",
    "images = []\n",
    "for sample in metadata.index:\n",
    "    filename = sample[:-3]+'jpg'\n",
    "    frame = grab_frames(sample, frame_num)\n",
    "    images_dict[filename] = frame\n",
    "    images.append(frame)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(frame_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(video_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 19\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 20\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 21\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 22\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 23\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 24\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 25\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 26\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 27\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 28\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 29\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 30\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 31\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 32\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 33\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 34\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 35\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 36\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 37\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 38\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 39\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 146.491 seconds\n",
      "3 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 40\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 257.697 seconds\n",
      "1 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 41\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 157.785 seconds\n",
      "1 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 42\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 156.675 seconds\n",
      "19 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 43\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 144.249 seconds\n",
      "2 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 44\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 154.130 seconds\n",
      "0 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 45\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 124.399 seconds\n",
      "9 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 46\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")\n",
    "\n",
    "del(faces)\n",
    "del(faces_dict)\n",
    "del(images)\n",
    "del(images_dict)\n",
    "del(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 147.767 seconds\n",
      "30 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 47\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")\n",
    "\n",
    "del(faces)\n",
    "del(faces_dict)\n",
    "del(images)\n",
    "del(images_dict)\n",
    "del(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 141.249 seconds\n",
      "9 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 48\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# parallel job \n",
    "# not iterating over all videos, iterating over a single video?\n",
    "results = Parallel(n_jobs=3)(delayed(\n",
    "                   grab_frames)(sample, frame_num)\n",
    "                       for sample in metadata.index)\n",
    "\n",
    "# unpack reults\n",
    "images, results_dict = zip(*results)\n",
    "images = list(images)\n",
    "images_dict = {}\n",
    "for i in results_dict:\n",
    "    images_dict.update(i)\n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "        #print(type(imgs_pil[0]))\n",
    "        try:\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")\n",
    "\n",
    "del(faces)\n",
    "del(faces_dict)\n",
    "del(images)\n",
    "del(images_dict)\n",
    "del(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TypeError: zip argument #2688 must support iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 49\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        reader.release()\n",
    "        return image\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "images_dict= {}\n",
    "images = []\n",
    "for sample in metadata.index:\n",
    "    filename = sample[:-3]+'jpg'\n",
    "    frame = grab_frames(sample, frame_num)\n",
    "    images_dict[filename] = frame\n",
    "    images.append(frame)\n",
    "    \n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        try:\n",
    "            imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "            #print(type(imgs_pil[0]))\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")\n",
    "\n",
    "del(faces)\n",
    "del(faces_dict)\n",
    "del(images)\n",
    "del(images_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 183.676 seconds\n",
      "11 frames without faces detected\n"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 18\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        reader.release()\n",
    "        return image\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "images_dict= {}\n",
    "images = []\n",
    "for sample in metadata.index:\n",
    "    filename = sample[:-3]+'jpg'\n",
    "    frame = grab_frames(sample, frame_num)\n",
    "    images_dict[filename] = frame\n",
    "    images.append(frame)\n",
    "    \n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        try:\n",
    "            imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "            #print(type(imgs_pil[0]))\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")\n",
    "\n",
    "del(faces)\n",
    "del(faces_dict)\n",
    "del(images)\n",
    "del(images_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------  \n",
    "# New Chunk 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error: zip argument #9 must support iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting faces in frames, 162.733 seconds\n",
      "14 frames without faces detected\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2eea441d7357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# set deepfake directory chunk\n",
    "chunk = 27\n",
    "# frame number to grab from videos\n",
    "frame_num = 99\n",
    "\n",
    "video_dir = f'/home/ec2-user/SageMaker/data/deepfake/dfdc_train_part_{chunk}'\n",
    "frame_dir = f'../data/frames/f{frame_num}'\n",
    "meta_file = os.path.join(video_dir, 'metadata.json')\n",
    "\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "with open(meta_file) as f:\n",
    "    metadata = pd.read_json(f).T\n",
    "\n",
    "#parallel processing cv2\n",
    "\n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        filename = sample[:-3]+'jpg'\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images_dict = {}\n",
    "        images_dict[filename] = image\n",
    "        reader.release()\n",
    "        return image, images_dict\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "def grab_frames(sample, frame_num):\n",
    "    try:\n",
    "        video = os.path.join(video_dir, sample)\n",
    "        reader = cv2.VideoCapture(video)\n",
    "        reader.set(1, frame_num)\n",
    "        _, image = reader.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        reader.release()\n",
    "        return image\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "images_dict= {}\n",
    "images = []\n",
    "for sample in metadata.index:\n",
    "    filename = sample[:-3]+'jpg'\n",
    "    frame = grab_frames(sample, frame_num)\n",
    "    images_dict[filename] = frame\n",
    "    images.append(frame)\n",
    "    \n",
    "    \n",
    "\n",
    "def timer(detector, detect_fn, images, *args):\n",
    "    start = time.time()\n",
    "    faces, faces_dict = detect_fn(detector, images, *args)\n",
    "    elapsed = time.time() - start\n",
    "    print(f', {elapsed:.3f} seconds')\n",
    "    return faces, elapsed, faces_dict\n",
    "\n",
    "# my attempt at adding a dictionary to the for loop to keep track of filenames\n",
    "\n",
    "detector = MTCNN(image_size=224, device=device, post_process=False)\n",
    "\n",
    "\n",
    "def detect_facenet_pytorch(detector, images, batch_size):\n",
    "    faces = []\n",
    "    faces_dict = {}\n",
    "    n = 0\n",
    "    for key in images_dict.keys():\n",
    "    #for lb in np.arange(0, len(images), batch_size):\n",
    "        try:\n",
    "            imgs_pil = [Image.fromarray(images_dict[key])]\n",
    "            #print(type(imgs_pil[0]))\n",
    "            faces.extend(detector(imgs_pil))\n",
    "            #print(len(faces))\n",
    "            faces_dict[key] = faces[n]\n",
    "            n += 1\n",
    "        except:\n",
    "            pass\n",
    "    return faces, faces_dict\n",
    "\n",
    "times_facenet_pytorch_nb = [] # non-batched\n",
    "\n",
    "# dtect faces from images\n",
    "print('Detecting faces in frames', end='')\n",
    "faces, elapsed, faces_dict = timer(detector, detect_facenet_pytorch, images, 1)\n",
    "times_facenet_pytorch_nb.append(elapsed)\n",
    "\n",
    "for face in faces_dict.keys():\n",
    "    try:\n",
    "        image = faces_dict[face].permute(1, 2, 0).int().numpy()\n",
    "        filename = os.path.join(frame_dir, face)\n",
    "        cv2.imwrite(filename, image)\n",
    "    except:\n",
    "        faces_dict[face] = 'no face detected'\n",
    "        pass\n",
    "    \n",
    "no_faces_detected = 0\n",
    "for face in faces_dict.keys():\n",
    "    if isinstance(faces_dict[face], str):\n",
    "        no_faces_detected += 1\n",
    "        \n",
    "print(f'{no_faces_detected} frames without faces detected')\n",
    "\n",
    "\n",
    "sns = boto3.client('sns')\n",
    "response = sns.publish(\n",
    "    TopicArn='arn:aws:sns:us-east-1:364430515305:deepfake',\n",
    "    Message=f' chunk #{chunk} frame#{frame_num} finished processing'\n",
    ")\n",
    "\n",
    "del(faces)\n",
    "del(faces_dict)\n",
    "del(images)\n",
    "del(images_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
