{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as t_F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "data_path = \"kaggle/input/deepfake-detection-challenge/test_videos\"\n",
    "save_model_path = \"kaggle/input/single-frame/\"\n",
    "meta_data = \"kaggle/input/deepfake-detection-challenge/train_sample_videos/metadata.json\"\n",
    "\n",
    "res_size = 224        # ResNet image size\n",
    "\n",
    "# training parameters\n",
    "k = 2             # number of target category\n",
    "epochs = 30        # training epochs\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "\n",
    "    def __init__(self, files, labels, num_frames, transform=None, test=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.files = files\n",
    "        self.labels  = labels\n",
    "        self.num_frames = num_frames\n",
    "        self.max_num_frames = 60\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        self.frame_no = num_frames\n",
    "        self.face_cascade = cv2.CascadeClassifier('kaggle/input/single-frame/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    def face_detect(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize frame of video to 1/4 size for faster face detection processing\n",
    "        small_frame = cv2.resize(gray, (0, 0), fx=0.25, fy=0.25)\n",
    "        # Detect the faces\n",
    "        faces = self.face_cascade.detectMultiScale(small_frame, 1.1, 4)\n",
    "        return faces\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "    def readVideo(self, videoFile):\n",
    "\n",
    "        # Load the cascade\n",
    "\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(videoFile)\n",
    "        # cap.set(1, self.frame_no)\n",
    "        # nFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # frames = torch.FloatTensor(self.channels, self.timeDepth, self.xSize, self.ySize)\n",
    "\n",
    "        attempts = 0\n",
    "        while attempts < self.max_num_frames:\n",
    "            ret, frame = cap.read()\n",
    "            attempts += 1\n",
    "            if ret:\n",
    "                last_good_frame = frame\n",
    "                try:\n",
    "                    faces = self.face_detect(frame)\n",
    "                    # Face detected\n",
    "                    if len(faces) > 0:\n",
    "                        # Get the face, if more than two, use the whole frame\n",
    "                        if len(faces) > 1:\n",
    "                            break\n",
    "                        x, y, w, h = faces[0] * 4\n",
    "                        face_img = frame[y: y + h, x: x + w]\n",
    "                        frame = torch.from_numpy(face_img)\n",
    "                        # HWC2CHW\n",
    "                        frame = frame.permute(2, 0, 1)\n",
    "                        if self.transform is not None:\n",
    "                            frame = t_F.to_pil_image(frame)\n",
    "                            frame = self.transform(frame)\n",
    "                            cap.release()\n",
    "                            return frame\n",
    "                except:\n",
    "                    print(\"Face detection error\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        frame = torch.from_numpy(last_good_frame)\n",
    "        # HWC2CHW\n",
    "        frame = frame.permute(2, 0, 1)\n",
    "        if self.transform is not None:\n",
    "            frame = t_F.to_pil_image(frame)\n",
    "            frame = self.transform(frame)\n",
    "        cap.release()\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file = self.files[index]\n",
    "        X = self.readVideo(file)\n",
    "        if self.test:\n",
    "            y = self.labels[index]\n",
    "        else:\n",
    "            y = torch.LongTensor([self.labels[index]])  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # set model as testing mode\n",
    "    output_file = 'submission.csv'\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)      \n",
    "    cnn_encoder = model\n",
    "    cnn_encoder.eval()\n",
    "\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X = X.to(device)\n",
    "            # y = y.to(device).view(-1, )\n",
    "            output = cnn_encoder(X)\n",
    "            output_prob = F.softmax(output, dim=1)\n",
    "            for i, item in enumerate(output_prob):\n",
    "                file_name = y[i].split('/')[-1]\n",
    "                #file_name = y[i] \n",
    "                prob = output_prob[i][1].item()\n",
    "                results[file_name] = prob\n",
    "                \n",
    "    df =  pd.DataFrame([results.keys(), results.values()]).T\n",
    "    df.columns = ['filename', 'label']\n",
    "    df.fillna(0.5)\n",
    "    df.to_csv(output_file, sep=',', index=False)\n",
    "    print(\"Finished prediction!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(data_folder, valid=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    videos = os.listdir(data_folder)\n",
    "    if valid:\n",
    "         with open(os.path.join(data_folder, meta_data)) as json_file:\n",
    "            label_data = json.load(json_file)\n",
    "    for v in videos:\n",
    "        if v.endswith('mp4'):\n",
    "            X.append(os.path.join(data_folder, v))\n",
    "            if valid:\n",
    "                if label_data[v]['label'] == 'FAKE':\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "test_X, test_y = get_X(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "num_frames = 60\n",
    "\n",
    "test_set = FrameDataset(test_X, test_X, num_frames, transform=transform, test=True)\n",
    "test_loader = data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "model_ft = models.resnet18()\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Load model\n",
    "encoder_model_path = os.path.join(save_model_path, 'cnn_encoder_epoch1.pth')\n",
    "model_ft.load_state_dict(torch.load(encoder_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction!!!\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "test(model_ft, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
