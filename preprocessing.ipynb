{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as t_F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# working with the sample videos\n",
    "# move the real and fake videos into sperate folders for training data\n",
    "\n",
    "train_root_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/train'\n",
    "train_real_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/train/real'\n",
    "train_fake_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/train/fake'\n",
    "train_label_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/train/metadata.json'\n",
    "\n",
    "train_videos = os.listdir(train_root_path)\n",
    "\n",
    "with open(train_label_path) as json_file:\n",
    "    train_label_data = json.load(json_file)\n",
    "\n",
    "for v in train_videos:\n",
    "    if v.endswith('mp4'):\n",
    "        if train_label_data[v]['label'] == 'FAKE':\n",
    "            # move to fake folder\n",
    "            shutil.move(os.path.join(train_root_path, v), train_fake_path)\n",
    "        else:\n",
    "            # move to real folder\n",
    "            shutil.move(os.path.join(train_root_path, v), train_real_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# working with the sample videos\n",
    "# move the real and fake videos into sperate folders for validation data\n",
    "\n",
    "val_root_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val'\n",
    "val_real_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/real'\n",
    "val_fake_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/fake'\n",
    "val_label_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/metadata.json'\n",
    "\n",
    "val_videos = os.listdir(val_root_path)\n",
    "\n",
    "with open(val_label_path) as json_file:\n",
    "    val_label_data = json.load(json_file)\n",
    "\n",
    "for v in val_videos:\n",
    "    if v.endswith('mp4'):\n",
    "        if val_label_data[v]['label'] == 'FAKE':\n",
    "            # move to fake folder\n",
    "            shutil.move(os.path.join(val_root_path, v), val_fake_path)\n",
    "        else:\n",
    "            # move to real folder\n",
    "            shutil.move(os.path.join(val_root_path, v), val_real_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# working with the sample videos\n",
    "# move the real and fake videos into sperate folders for validation data\n",
    "\n",
    "val_root_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val'\n",
    "val_real_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/real'\n",
    "val_fake_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/fake'\n",
    "val_label_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/metadata.json'\n",
    "\n",
    "val_videos = os.listdir(val_root_path)\n",
    "\n",
    "with open(val_label_path) as json_file:\n",
    "    val_label_data = json.load(json_file)\n",
    "\n",
    "for v in val_videos:\n",
    "    if v.endswith('mp4'):\n",
    "        if val_label_data[v]['label'] == 'FAKE':\n",
    "            # move to fake folder\n",
    "            shutil.move(os.path.join(val_root_path, v), val_fake_path)\n",
    "        else:\n",
    "            # move to real folder\n",
    "            shutil.move(os.path.join(val_root_path, v), val_real_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with the sample videos\n",
    "# move the real and fake videos into sperate folders for validation data\n",
    "\n",
    "val_root_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val'\n",
    "val_real_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/real'\n",
    "val_fake_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/fake'\n",
    "val_label_path = '/home/ec2-user/SageMaker/data/deepfake/sample_videos/val/metadata.json'\n",
    "\n",
    "val_videos = os.listdir(val_root_path)\n",
    "\n",
    "with open(val_label_path) as json_file:\n",
    "    val_label_data = json.load(json_file)\n",
    "\n",
    "for v in val_videos:\n",
    "    if v.endswith('mp4'):\n",
    "        if val_label_data[v]['label'] == 'FAKE':\n",
    "            # move to fake folder\n",
    "            shutil.move(os.path.join(val_root_path, v), val_fake_path)\n",
    "        else:\n",
    "            # move to real folder\n",
    "            shutil.move(os.path.join(val_root_path, v), val_real_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the training videos into raw frames\n",
    "\n",
    "# variables\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "raw_train_real_path = '/home/ec2-user/SageMaker/data/deepfake/raw_frames/train/real'\n",
    "raw_train_fake_path = '/home/ec2-user/SageMaker/data/deepfake/raw_frames/train/fake'\n",
    "raw_val_real_path = '/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/real'\n",
    "raw_val_fake_path = '/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/fake'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([224, 224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "raw_train_real_list = os.listdir(train_real_path)\n",
    "raw_train_fake_list = os.listdir(train_fake_path)\n",
    "raw_val_real_list = os.listdir(val_real_path)\n",
    "raw_val_fake_list = os.listdir(val_fake_path)\n",
    "\n",
    "temp_list = []\n",
    "for video in raw_train_real_list:\n",
    "    temp_list.append(os.path.join(train_real_path, video))\n",
    "raw_train_real_list = temp_list\n",
    "\n",
    "temp_list = []\n",
    "for video in raw_train_fake_list:\n",
    "    temp_list.append(os.path.join(train_fake_path, video))\n",
    "raw_train_fake_list = temp_list\n",
    "\n",
    "temp_list = []\n",
    "for video in raw_val_real_list:\n",
    "    temp_list.append(os.path.join(val_real_path, video))\n",
    "raw_val_real_list = temp_list\n",
    "\n",
    "temp_list = []\n",
    "for video in raw_val_fake_list:\n",
    "    temp_list.append(os.path.join(val_fake_path, video))\n",
    "raw_val_fake_list = temp_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in raw_train_real_list:\n",
    "    count = 1\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    ret,image = vidcap.read()\n",
    "    # read videos and write images (300 images for a 10 sec clip)\n",
    "    while ret:\n",
    "        # cv2 is not taking os.path.join properly. it is writing jpg's to the same directory where the videos are\n",
    "        # had to manually copy jpgs from video path to raw folder\n",
    "        cv2.imwrite(os.path.join(raw_train_real_path, video.strip('.mp4')+'_'+str(count)+'.jpg'), image)     # save frame as JPEG file      \n",
    "        ret,image = vidcap.read()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in raw_train_fake_list:\n",
    "    count = 1\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    ret,image = vidcap.read()\n",
    "    # read videos and write images (300 images for a 10 sec clip)\n",
    "    while ret:\n",
    "        # cv2 is not taking os.path.join properly. it is writing jpg's to the same directory where the videos are\n",
    "        # had to manually copy jpgs from video path to raw folder\n",
    "        cv2.imwrite(os.path.join(raw_train_fake_path, video.strip('.mp4')+'_'+str(count)+'.jpg'), image)     # save frame as JPEG file      \n",
    "        ret,image = vidcap.read()\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "for video in raw_val_real_list:\n",
    "    count = 1\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    ret,image = vidcap.read()\n",
    "    # read videos and write images (300 images for a 10 sec clip)\n",
    "    while ret:\n",
    "        # cv2 is not taking os.path.join properly. it is writing jpg's to the same directory where the videos are\n",
    "        # had to manually copy jpgs from video path to raw folder\n",
    "        cv2.imwrite(os.path.join(raw_val_real_path, video.strip('.mp4')+'_'+str(count)+'.jpg'), image)     # save frame as JPEG file      \n",
    "        ret,image = vidcap.read()\n",
    "        count += 1\n",
    "        \n",
    "for video in raw_val_fake_list:\n",
    "    count = 1\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    ret,image = vidcap.read()\n",
    "    # read videos and write images (300 images for a 10 sec clip)\n",
    "    while ret:\n",
    "        # cv2 is not taking os.path.join properly. it is writing jpg's to the same directory where the videos are\n",
    "        # had to manually copy jpgs from video path to raw folder\n",
    "        cv2.imwrite(os.path.join(raw_val_fake_path, video.strip('.mp4')+'_'+str(count)+'.jpg'), image)     # save frame as JPEG file      \n",
    "        ret,image = vidcap.read()\n",
    "        count += 1\n",
    "        \n",
    "#\n",
    "#\n",
    "#\n",
    "#       COPY JPGs FROM VIDEO FOLDERS TO RAW IMAGE FOLDERS\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_train_real_list = os.listdir(raw_train_real_path)\n",
    "raw_image_train_fake_list = os.listdir(raw_train_fake_path)\n",
    "\n",
    "raw_image_val_real_list = os.listdir(raw_val_real_path)\n",
    "raw_image_val_fake_list = os.listdir(raw_val_fake_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for image in raw_image_train_real_list:\n",
    "    temp_list.append(os.path.join(raw_train_real_path, image))\n",
    "raw_image_train_real_list = temp_list\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "for image in raw_image_train_fake_list:\n",
    "    temp_list.append(os.path.join(raw_train_fake_path, image))\n",
    "raw_image_train_fake_list = temp_list\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "for image in raw_image_val_real_list:\n",
    "    temp_list.append(os.path.join(raw_val_real_path, image))\n",
    "raw_image_val_real_list = temp_list\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "for image in raw_image_val_fake_list:\n",
    "    temp_list.append(os.path.join(raw_val_fake_path, image))\n",
    "raw_image_val_fake_list = temp_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/real/oupztxzdln_43.jpg',\n",
       " '/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/real/arwiyweadv_157.jpg',\n",
       " '/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/real/dusbhhockt_261.jpg',\n",
       " '/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/real/pppakyhsqd_131.jpg',\n",
       " '/home/ec2-user/SageMaker/data/deepfake/raw_frames/val/real/sleykxpevy_128.jpg']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_image_val_real_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect faces, and apply transforms for resent transfer learning\n",
    "#\n",
    "\n",
    "# process images by: cropping faces\n",
    "def face_detect(frame):\n",
    "    dir = '/home/ec2-user/SageMaker/data/deepfake/processed/train/real'\n",
    "    name = frame.split('/')[-1]\n",
    "    frame = cv2.imread(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(gray, (0, 0), fx=0.25, fy=0.25)\n",
    "    # Detect the faces\n",
    "    faces = face_cascade.detectMultiScale(small_frame, 1.1, 4)\n",
    "    x, y, w, h = faces[0] * 4\n",
    "    face_img = frame[y: y + h, x: x + w]\n",
    "    frame = torch.from_numpy(face_img)\n",
    "    # HWC2CHW\n",
    "    frame = frame.permute(2, 0, 1)\n",
    "    frame = t_F.to_pil_image(frame)\n",
    "    frame = transform(frame)\n",
    "    write_to_disk = save_image(frame, os.path.join(dir, name))\n",
    "    return\n",
    "\n",
    "# start the processing job!\n",
    "#\n",
    "# for TRAIN REAL images\n",
    "#\n",
    "#\n",
    "for image in raw_image_train_real_list:\n",
    "    try:\n",
    "        face_detect(image)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect faces, and apply transforms for resent transfer learning\n",
    "#\n",
    "\n",
    "processed_train_real = '/home/ec2-user/SageMaker/data/deepfake/processed/train/real'\n",
    "processed_train_fake = '/home/ec2-user/SageMaker/data/deepfake/processed/train/fake'\n",
    "\n",
    "processed_val_real = '/home/ec2-user/SageMaker/data/deepfake/processed/val/real'\n",
    "processed_val_fake = '/home/ec2-user/SageMaker/data/deepfake/processed/val/fake'\n",
    "\n",
    "# process images by: cropping faces\n",
    "def face_detect(output_dir, frame):\n",
    "    name = frame.split('/')[-1]\n",
    "    frame = cv2.imread(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize frame of video to 1/4 size for faster face detection processing\n",
    "    small_frame = cv2.resize(gray, (0, 0), fx=0.25, fy=0.25)\n",
    "    # Detect the faces\n",
    "    faces = face_cascade.detectMultiScale(small_frame, 1.1, 4)\n",
    "    x, y, w, h = faces[0] * 4\n",
    "    face_img = frame[y: y + h, x: x + w]\n",
    "    frame = torch.from_numpy(face_img)\n",
    "    # HWC2CHW\n",
    "    frame = frame.permute(2, 0, 1)\n",
    "    frame = t_F.to_pil_image(frame)\n",
    "    frame = transform(frame)\n",
    "    write_to_disk = save_image(frame, os.path.join(output_dir, name))\n",
    "    return\n",
    "\n",
    "# start the processing job!\n",
    "#\n",
    "# for TRAIN FAKE images\n",
    "#\n",
    "#\n",
    "for image in raw_image_train_fake_list:\n",
    "    try:\n",
    "        face_detect(processed_train_fake, image)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "# start the processing job!\n",
    "#\n",
    "# for VAL REAL images\n",
    "#\n",
    "#\n",
    "for image in raw_image_val_real_list:\n",
    "    try:\n",
    "        face_detect(processed_val_real, image)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "# start the processing job!\n",
    "#\n",
    "# for VAL REAL images\n",
    "#\n",
    "#\n",
    "for image in raw_image_val_fake_list:\n",
    "    try:\n",
    "        face_detect(processed_val_fake, image)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "real image count: 17384\n",
      "fake image count: 70335\n",
      "\n",
      "validation set\n",
      "real image count: 3895\n",
      "fake image count: 20865\n"
     ]
    }
   ],
   "source": [
    "print('training set')\n",
    "print('real image count: {}'.format(len(os.listdir(processed_train_real))))\n",
    "print('fake image count: {}'.format(len(os.listdir(processed_train_fake))))\n",
    "print('')\n",
    "print('validation set')\n",
    "print('real image count: {}'.format(len(os.listdir(processed_val_real))))\n",
    "print('fake image count: {}'.format(len(os.listdir(processed_val_fake))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
