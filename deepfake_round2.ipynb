{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as t_F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "train_data_path = '/home/ec2-user/SageMaker/pytorch_learning/data/deepfake/train'\n",
    "val_data_path = '/home/ec2-user/SageMaker/pytorch_learning/data/deepfake/val'\n",
    "meta_data = 'metadata.json'\n",
    "val_meta_data_path = '/home/ec2-user/.fastai/data/deepfake/dfdc_train_part_49'\n",
    "save_model_path = \"./\"\n",
    "\n",
    "res_size = 224        # ResNet image size\n",
    "\n",
    "# training parameters\n",
    "k = 2             # number of target category\n",
    "epochs = 30        # training epochs\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info\n",
    "\n",
    "\n",
    "# from pytorch tutorial\n",
    "dataset_sizes = {'train': len(os.listdir(train_data_path)), 'val': len(os.listdir(val_data_path)) }\n",
    "class_names = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "\n",
    "    def __init__(self, files, labels, num_frames, transform=None, test=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.files = files\n",
    "        self.labels  = labels\n",
    "        self.num_frames = num_frames\n",
    "        self.max_num_frames = 60\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        self.frame_no = num_frames\n",
    "        self.face_cascade = cv2.CascadeClassifier('/home/ec2-user/SageMaker/kaggle/input/single-frame/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    def face_detect(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize frame of video to 1/4 size for faster face detection processing\n",
    "        small_frame = cv2.resize(gray, (0, 0), fx=0.25, fy=0.25)\n",
    "        # Detect the faces\n",
    "        faces = self.face_cascade.detectMultiScale(small_frame, 1.1, 4)\n",
    "        return faces\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "    def readVideo(self, videoFile):\n",
    "\n",
    "        # Load the cascade\n",
    "\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(videoFile)\n",
    "        # cap.set(1, self.frame_no)\n",
    "        # nFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # frames = torch.FloatTensor(self.channels, self.timeDepth, self.xSize, self.ySize)\n",
    "\n",
    "        attempts = 0\n",
    "        while attempts < self.max_num_frames:\n",
    "            ret, frame = cap.read()\n",
    "            attempts += 1\n",
    "            if ret:\n",
    "                last_good_frame = frame\n",
    "                try:\n",
    "                    faces = self.face_detect(frame)\n",
    "                    # Face detected\n",
    "                    if len(faces) > 0:\n",
    "                        # Get the face, if more than two, use the whole frame\n",
    "                        if len(faces) > 1:\n",
    "                            break\n",
    "                        x, y, w, h = faces[0] * 4\n",
    "                        face_img = frame[y: y + h, x: x + w]\n",
    "                        frame = torch.from_numpy(face_img)\n",
    "                        # HWC2CHW\n",
    "                        frame = frame.permute(2, 0, 1)\n",
    "                        if self.transform is not None:\n",
    "                            frame = t_F.to_pil_image(frame)\n",
    "                            frame = self.transform(frame)\n",
    "                            cap.release()\n",
    "                            return frame\n",
    "                except:\n",
    "                    print(\"Face detection error\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        frame = torch.from_numpy(last_good_frame)\n",
    "        # HWC2CHW\n",
    "        frame = frame.permute(2, 0, 1)\n",
    "        if self.transform is not None:\n",
    "            frame = t_F.to_pil_image(frame)\n",
    "            frame = self.transform(frame)\n",
    "        cap.release()\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file = self.files[index]\n",
    "        X = self.readVideo(file)\n",
    "        if self.test:\n",
    "            y = self.labels[index]\n",
    "        else:\n",
    "            y = torch.LongTensor([self.labels[index]])  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(data_folder, valid=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    videos = os.listdir(data_folder)\n",
    "    if valid:\n",
    "         with open(os.path.join(data_folder, meta_data)) as json_file:\n",
    "            label_data = json.load(json_file)\n",
    "    for v in videos:\n",
    "        if v.endswith('mp4'):\n",
    "            X.append(os.path.join(data_folder, v))\n",
    "            if valid:\n",
    "                if label_data[v]['label'] == 'FAKE':\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_V(data_folder, valid=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    videos = os.listdir(data_folder)\n",
    "    if valid:\n",
    "         with open(os.path.join(val_meta_data_path, meta_data)) as json_file:\n",
    "            label_data = json.load(json_file)\n",
    "    \n",
    "    # only keep 125 items in the label file and delete the rest\n",
    "    for k, v in list(label_data.items()):\n",
    "        if k not in videos:\n",
    "            del(label_data[k])\n",
    "        \n",
    "    for v in videos:\n",
    "        if v.endswith('mp4'):\n",
    "            X.append(os.path.join(data_folder, v))\n",
    "            if valid:\n",
    "                if label_data[v]['label'] == 'FAKE':\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_X(train_data_path, valid=True)\n",
    "val_x, val_y = get_V(val_data_path, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "num_frames = 60\n",
    "res_size = 224\n",
    "\n",
    "train_set = FrameDataset(train_x, train_y, num_frames, transform=transform)\n",
    "val_set = FrameDataset(val_x, val_y, num_frames, transform=transform)\n",
    "dataloaders = { \"train\": data.DataLoader(train_set, **params), \"val\": data.DataLoader(val_set, **params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
